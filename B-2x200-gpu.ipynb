{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #make forecast dictionary from all available training data\n",
    "# THIS_DIR = getcwd()\n",
    "# SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "# WDATA_DIR = abspath(join(THIS_DIR, 'weather_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "\n",
    "# NO_OF_FCAST_TYPES = 10\n",
    "# FCAST_DICT = {}\n",
    "\n",
    "# for location in ['tokyo','wakkanai','minamidaito']:\n",
    "#     for year in np.arange(2000,2010): #2017 and 2018 for minamidaito are corrupted\n",
    "#         sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "#         wfile = WDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "\n",
    "#         #skiprows=4 to remove unnecessary title texts\n",
    "#         #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "#         solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "#         fcast_am = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[3])\n",
    "# #         fcast_pm = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[6])\n",
    "\n",
    "#         #convert dataframe to numpy array\n",
    "#         solar_radiation = solar_radiation.values\n",
    "\n",
    "#         #convert missing data in CSV files to zero\n",
    "#         solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "#         #reshape solar_radiation into no_of_daysx24 array\n",
    "#         solar_radiation = solar_radiation.reshape(-1,24)\n",
    "#         day_radiation = np.sum(solar_radiation,axis=1)\n",
    "#         day_radiation_rec = np.append(day_radiation_rec,day_radiation)\n",
    "        \n",
    "#         #get forecast data, flatten the array and convert to list\n",
    "#         fcast_am = fcast_am.values.flatten().tolist()\n",
    "#         fcast_am = [item.replace('\\u3000','') for item in fcast_am] #remove pesky UNICODE character\n",
    "#         fcast_rec.extend(fcast_am)\n",
    "        \n",
    "# # Create forecast dictionary using forecast from 06:00 to 18:00\n",
    "# # Map every forecast string to forecast type\n",
    "# counts, bin_edges = np.histogram(day_radiation_rec, bins=NO_OF_FCAST_TYPES);\n",
    "# for forecast, tot_day_radiation in zip(fcast_rec, day_radiation_rec):\n",
    "#     for k in np.arange(1,bin_edges.size):\n",
    "#         if (bin_edges[k-1] < tot_day_radiation < bin_edges[k]):\n",
    "#             FCAST_DICT[forecast] = k -1\n",
    "            \n",
    "            \n",
    "# #Alternate forecast method without using forecast CSV data\n",
    "# daytype_rec = []\n",
    "# counts, bin_edges = np.histogram(day_radiation_rec, bins=NO_OF_FCAST_TYPES);\n",
    "# for tot_day_radiation in day_radiation_rec:\n",
    "#     for k in np.arange(1,bin_edges.size):\n",
    "#         if (bin_edges[k-1] <= tot_day_radiation <= bin_edges[k]):\n",
    "#             daytype_rec = np.append(daytype_rec, k-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 5 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "#         WDATA_DIR = abspath(join(THIS_DIR, 'weather_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "#         wfile = WDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "        #usecols=3 to read only the weather summary from 06:00 to 18:00\n",
    "#         fcast_am = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[3])\n",
    "        #usecols=6 to read only the weather summary from 18:00 to 06:00+1\n",
    "        #fcast_pm = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[6])\n",
    "\n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "        \n",
    "        \n",
    "#         #get weather summary data, flatten the array and convert to list\n",
    "#         fcast_am = fcast_am.values.flatten().tolist()\n",
    "#         fcast_am = [item.replace('\\u3000','') for item in fcast_am] #remove pesky UNICODE character\n",
    "        \n",
    "#         #convert weather summary to forecast type using dictionary fcast_dict{}\n",
    "#         self.fforecast = np.array([fcast_dict[x] for x in fcast_am])\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        if (tot_day_radiation < 3.5):\n",
    "            day_state = 0\n",
    "        elif (3.5 <= tot_day_radiation < 7):\n",
    "            day_state = 1\n",
    "        elif (7 <= tot_day_radiation < 12):\n",
    "            day_state = 2\n",
    "        elif (12 <= tot_day_radiation < 15):\n",
    "            day_state = 3\n",
    "        elif (15 <= tot_day_radiation < 17.5):\n",
    "            day_state = 4\n",
    "        else:\n",
    "            day_state = 5\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.no_of_day_state = 6;\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "        self.enp = self.BOPT - self.batt\n",
    "#         self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "#         norm_fcast = self.fcast/(self.no_of_day_state-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "#         norm_fcast = self.fcast/(self.no_of_day_state-1)\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "    def rewardfn(self):\n",
    "        R_PARAM = 20000 #chosen empirically for best results\n",
    "        mu = 0\n",
    "        sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "        norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "#         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "#             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "#         else:\n",
    "#             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "        if(self.violation_flag):\n",
    "            norm_reward -= 0.5\n",
    "            \n",
    "        return (norm_reward)\n",
    "        \n",
    "    \n",
    "#     #reward function\n",
    "#     def rewardfn(self):\n",
    "        \n",
    "#         #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "#         bmean = np.mean(self.btrack)\n",
    "#         bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "#         # based on the sigmoid function\n",
    "#         # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "#         p1_sharpness = 10\n",
    "#         n1_sharpness = 20\n",
    "#         shift1 = 0.5\n",
    "#         # r1(x) = 0.5 when x = 0.25. \n",
    "#         # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "#         if(bdev<=0.25): \n",
    "#             r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "#         else: \n",
    "#             r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "#         # r1 ranges from -1 to 1\n",
    "            \n",
    "#         #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "#         if(bdev <=0.25):\n",
    "#             # enp ranges from enp = (0,3) of DMAX\n",
    "#             p2_sharpness = 2\n",
    "#             n2_sharpness = 2\n",
    "#             shift2 = 6    \n",
    "#             # r1(x) = 0.5 when x = 2. \n",
    "#             # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "# #             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "# #             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "# #             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "#             x = np.abs(self.enp/self.DMAX)\n",
    "#             if(x<=2): \n",
    "#                 r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "#             else: \n",
    "#                 r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "#         else:\n",
    "#             r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "#         # r2 ranges from 0 to 1\n",
    "\n",
    "#         #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "#         if(self.violation_flag):\n",
    "#             violation_penalty = 2\n",
    "#         else:\n",
    "#             violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "# #         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "#         return (r1 + r2 - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt <= self.BMIN or self.batt >= self.BMAX ):\n",
    "            if(self.violation_flag == False): \n",
    "                self.violation_counter += 1\n",
    "            self.violation_flag = True #penalty for violating battery limits anytime during the day\n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "        self.enp = self.BOPT - self.batt \n",
    "#         self.enp = self.binit - self.batt\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "\n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward = self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                self.batt = self.BOPT  \n",
    "            \n",
    "            self.violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/5\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*3      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 3 #number of state space parameter [batt, enp, henergy]\n",
    "\n",
    "HIDDEN_LAYER = 200\n",
    "NO_OF_ITERATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        self.fc1.weight.data.normal_(0, 0.01)   # initialization\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.fc2.weight.data.normal_(0, 0.01)   # initialization\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.01)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "    \n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] # return the argmax index\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.30\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    norm_batt, norm_enp, norm_henergy = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Iteration 0:  TOKYO, 2009 \n",
      "EPSILON = 0.5, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=   -0.878\n",
      "Violation Counter \t= 199\n",
      "Iteration 1:  TOKYO, 2009 \n",
      "EPSILON = 0.5, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=   -0.866\n",
      "Violation Counter \t= 201\n",
      "Iteration 2:  TOKYO, 2009 \n",
      "EPSILON = 0.5, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=   -0.589\n",
      "Violation Counter \t= 144\n",
      "Iteration 3:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.546\n",
      "Violation Counter \t= 19\n",
      "Iteration 4:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.384\n",
      "Violation Counter \t= 60\n",
      "Iteration 5:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.382\n",
      "Violation Counter \t= 62\n",
      "Iteration 6:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.513\n",
      "Violation Counter \t= 39\n",
      "Iteration 7:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.955\n",
      "Violation Counter \t= 42\n",
      "Iteration 8:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.114\n",
      "Violation Counter \t= 39\n",
      "Iteration 9:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.066\n",
      "Violation Counter \t= 53\n",
      "Iteration 10:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.757\n",
      "Violation Counter \t= 52\n",
      "Iteration 11:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.391\n",
      "Violation Counter \t= 42\n",
      "Iteration 12:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.458\n",
      "Violation Counter \t= 45\n",
      "Iteration 13:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.689\n",
      "Violation Counter \t= 40\n",
      "Iteration 14:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.580\n",
      "Violation Counter \t= 50\n",
      "Iteration 15:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.731\n",
      "Violation Counter \t= 29\n",
      "Iteration 16:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.709\n",
      "Violation Counter \t= 50\n",
      "Iteration 17:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.776\n",
      "Violation Counter \t= 46\n",
      "Iteration 18:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.930\n",
      "Violation Counter \t= 34\n",
      "Iteration 19:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.784\n",
      "Violation Counter \t= 47\n",
      "Iteration 20:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.747\n",
      "Violation Counter \t= 68\n",
      "Iteration 21:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.559\n",
      "Violation Counter \t= 35\n",
      "Iteration 22:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.887\n",
      "Violation Counter \t= 42\n",
      "Iteration 23:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.803\n",
      "Violation Counter \t= 33\n",
      "Iteration 24:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.777\n",
      "Violation Counter \t= 43\n",
      "Iteration 25:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.029\n",
      "Violation Counter \t= 45\n",
      "Iteration 26:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.782\n",
      "Violation Counter \t= 32\n",
      "Iteration 27:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.747\n",
      "Violation Counter \t= 34\n",
      "Iteration 28:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.849\n",
      "Violation Counter \t= 34\n",
      "Iteration 29:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.641\n",
      "Violation Counter \t= 28\n",
      "Iteration 30:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.418\n",
      "Violation Counter \t= 36\n",
      "Iteration 31:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.751\n",
      "Violation Counter \t= 24\n",
      "Iteration 32:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.826\n",
      "Violation Counter \t= 31\n",
      "Iteration 33:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.974\n",
      "Violation Counter \t= 29\n",
      "Iteration 34:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.041\n",
      "Violation Counter \t= 43\n",
      "Iteration 35:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.895\n",
      "Violation Counter \t= 40\n",
      "Iteration 36:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.928\n",
      "Violation Counter \t= 36\n",
      "Iteration 37:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.962\n",
      "Violation Counter \t= 35\n",
      "Iteration 38:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.752\n",
      "Violation Counter \t= 30\n",
      "Iteration 39:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.091\n",
      "Violation Counter \t= 80\n",
      "Iteration 40:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.352\n",
      "Violation Counter \t= 86\n",
      "Iteration 41:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.529\n",
      "Violation Counter \t= 81\n",
      "Iteration 42:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.540\n",
      "Violation Counter \t= 81\n",
      "Iteration 43:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.603\n",
      "Violation Counter \t= 60\n",
      "Iteration 44:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.707\n",
      "Violation Counter \t= 30\n",
      "Iteration 45:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.756\n",
      "Violation Counter \t= 28\n",
      "Iteration 46:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.941\n",
      "Violation Counter \t= 26\n",
      "Iteration 47:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.808\n",
      "Violation Counter \t= 28\n",
      "Iteration 48:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.837\n",
      "Violation Counter \t= 25\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.859\n",
      "Violation Counter \t= 18\n"
     ]
    }
   ],
   "source": [
    "#TRAIN USING data from TOKYO, WAKKANAI, MINAMIDAITO  FROM 2005 to 2015 on pre-trained model\n",
    "#Simulating actual working\n",
    "#No Random Battery \n",
    "#No battery reset\n",
    "#No Shuffle\n",
    "#No day_balance\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Device used: ' , device)\n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "# old = dqn.eval_net.fc1.weight.data.numpy().flatten()\n",
    "# old2 = old\n",
    "\n",
    "# old2nd = dqn.eval_net.fc2.weight.data.numpy().flatten()\n",
    "# old2nd2 = old2nd\n",
    "\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_flag = True #put CAPM in traimode\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "Y_EAR = 2004\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    if (iteration < MEMORY_CAPACITY/(24*7*4*12) ):\n",
    "        EPSILON = 0.5\n",
    "    else:\n",
    "        EPSILON = 0.9\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = 2009#random.choice(np.arange(2005,2015))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=reset_flag) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('Iteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    print('EPSILON = {}, LR = {}, RST_FLAG = {}'.format(EPSILON, LR, reset_flag))\n",
    "\n",
    "    \n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "                                                       # However, we are interested only in the reward\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] = r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "#             if (capm.BMIN == capm.batt or capm.BMAX == capm.batt):\n",
    "#                 capm.batt = capm.BOPT\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "#             print(\"Learning:\",capm.eno.year, capm.eno.day)\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "    #             print(\"End of Year\")\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[reward_rec != 0] #remove all zero rewards in the middle of the days\n",
    "    print(\"Average Reward \\t\\t= {:8.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_reward_rec = yr_record[:,2]\n",
    "    yr_reward_rec = yr_reward_rec[yr_reward_rec != 0]\n",
    "   \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "    \n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "    \n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     plt.show()\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax0 = fig.add_subplot(131)\n",
    "#     ax0.plot(yr_reward_rec, color='g')\n",
    "    \n",
    "#     ax1 = fig.add_subplot(132)\n",
    "#     new = dqn.eval_net.fc1.weight.data.numpy().flatten()\n",
    "#     ax1.plot(old2,color='r', alpha=0.4)\n",
    "#     ax1.plot(old,color='r')\n",
    "#     ax1.plot(new,color='b')\n",
    "#     old2 = old\n",
    "#     old = new\n",
    "    \n",
    "#     ax2 = fig.add_subplot(133)\n",
    "#     new2nd = dqn.eval_net.fc2.weight.data.numpy().flatten()\n",
    "#     ax2.plot(old2nd2,color='r', alpha=0.4)\n",
    "#     ax2.plot(old2nd,color='r')\n",
    "#     ax2.plot(new2nd,color='b')\n",
    "#     old2nd2 = old2nd\n",
    "#     old2nd = new2nd\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training\n",
    "# Save the last model\n",
    "# torch.save(dqn.eval_net.state_dict(), TFILENAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,365]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0:20:34.692271\n"
     ]
    }
   ],
   "source": [
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #BFILENAME -> loads the best model\n",
    "# #TFILENAME -> loads the last model\n",
    "# MODELFILE = BFILENAME\n",
    "# TEST_LOCATION = 'tokyo'\n",
    "# dqn = DQN()\n",
    "# for TEST_YEAR in np.arange(2015,2018):\n",
    "#     test_model(TEST_LOCATION, TEST_YEAR, MODELFILE, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #BFILENAME -> loads the best model\n",
    "# #TFILENAME -> loads the last model\n",
    "# MODELFILE = TFILENAME\n",
    "# TEST_LOCATION = 'tokyo'\n",
    "# dqn = DQN()\n",
    "# for TEST_YEAR in np.arange(2016,2018)\n",
    "#     test_model(TEST_LOCATION, TEST_YEAR, MODELFILE, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_YEAR = 2018\n",
    "# for TEST_LOCATION in ['tokyo','wakkanai','minamidaito']:\n",
    "#     test_model(TEST_LOCATION, TEST_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn = DQN()\n",
    "# capm = CAPM(TEST_LOCATION,TEST_YEAR, shuffle=False, trainmode=False) #if reset is True\n",
    "# capm.eno = ENO(TEST_LOCATION,TEST_YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "# capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "\n",
    "# # load the required model\n",
    "# dqn.eval_net.load_state_dict(torch.load(MODELFILE))\n",
    "# dqn.eval_net.eval()\n",
    "# print('Model Used: ',MODELFILE)\n",
    "\n",
    "# s, r, day_end, year_end = capm.reset()\n",
    "# yr_test_record = np.empty(4)\n",
    "\n",
    "# while True:\n",
    "#     a = dqn.choose_greedy_action(s)\n",
    "\n",
    "#     #state = [batt, enp, henergy, fcast]\n",
    "#     yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "#     # take action\n",
    "#     s_, r, day_end, year_end = capm.step(a)\n",
    "    \n",
    "# #     if day_end:\n",
    "# #         if (capm.BMIN ==capm.batt or capm.batt == capm.BMAX):\n",
    "# #             capm.batt = capm.BOPT\n",
    "        \n",
    "\n",
    "#     if year_end:\n",
    "#         print(\"End of Test\")\n",
    "#         break\n",
    "\n",
    "#     s = s_\n",
    "\n",
    "# yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "# #Plot the reward and battery for the entire year run\n",
    "# title = TEST_LOCATION.upper() + ',' + str(TEST_YEAR)\n",
    "\n",
    "# NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "# yr_test_reward_rec = yr_test_record[:,2]\n",
    "# yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "# print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(24,10))\n",
    "# fig.suptitle(title, fontsize=15)\n",
    "\n",
    "# #     ax1 = fig.add_subplot(211)\n",
    "# #     ax1.plot(yr_test_reward_rec)\n",
    "# #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "# #     ax1.set_ylim([-3,1])\n",
    "\n",
    "# ax2 = fig.add_subplot(111)\n",
    "# ax2.plot(yr_test_record[:,0],'r')\n",
    "# ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "# ax2.set_ylim([0,1])\n",
    "# plt.sca(ax2)\n",
    "# plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = TEST_LOCATION.upper() + ',' + str(TEST_YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(150,170):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*yr_test_record[START,0],'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
