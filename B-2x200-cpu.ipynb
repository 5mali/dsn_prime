{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #make forecast dictionary from all available training data\n",
    "# THIS_DIR = getcwd()\n",
    "# SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "# WDATA_DIR = abspath(join(THIS_DIR, 'weather_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "\n",
    "# NO_OF_FCAST_TYPES = 10\n",
    "# FCAST_DICT = {}\n",
    "\n",
    "# for location in ['tokyo','wakkanai','minamidaito']:\n",
    "#     for year in np.arange(2000,2010): #2017 and 2018 for minamidaito are corrupted\n",
    "#         sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "#         wfile = WDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "\n",
    "#         #skiprows=4 to remove unnecessary title texts\n",
    "#         #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "#         solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "#         fcast_am = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[3])\n",
    "# #         fcast_pm = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[6])\n",
    "\n",
    "#         #convert dataframe to numpy array\n",
    "#         solar_radiation = solar_radiation.values\n",
    "\n",
    "#         #convert missing data in CSV files to zero\n",
    "#         solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "#         #reshape solar_radiation into no_of_daysx24 array\n",
    "#         solar_radiation = solar_radiation.reshape(-1,24)\n",
    "#         day_radiation = np.sum(solar_radiation,axis=1)\n",
    "#         day_radiation_rec = np.append(day_radiation_rec,day_radiation)\n",
    "        \n",
    "#         #get forecast data, flatten the array and convert to list\n",
    "#         fcast_am = fcast_am.values.flatten().tolist()\n",
    "#         fcast_am = [item.replace('\\u3000','') for item in fcast_am] #remove pesky UNICODE character\n",
    "#         fcast_rec.extend(fcast_am)\n",
    "        \n",
    "# # Create forecast dictionary using forecast from 06:00 to 18:00\n",
    "# # Map every forecast string to forecast type\n",
    "# counts, bin_edges = np.histogram(day_radiation_rec, bins=NO_OF_FCAST_TYPES);\n",
    "# for forecast, tot_day_radiation in zip(fcast_rec, day_radiation_rec):\n",
    "#     for k in np.arange(1,bin_edges.size):\n",
    "#         if (bin_edges[k-1] < tot_day_radiation < bin_edges[k]):\n",
    "#             FCAST_DICT[forecast] = k -1\n",
    "            \n",
    "            \n",
    "# #Alternate forecast method without using forecast CSV data\n",
    "# daytype_rec = []\n",
    "# counts, bin_edges = np.histogram(day_radiation_rec, bins=NO_OF_FCAST_TYPES);\n",
    "# for tot_day_radiation in day_radiation_rec:\n",
    "#     for k in np.arange(1,bin_edges.size):\n",
    "#         if (bin_edges[k-1] <= tot_day_radiation <= bin_edges[k]):\n",
    "#             daytype_rec = np.append(daytype_rec, k-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 5 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "#         WDATA_DIR = abspath(join(THIS_DIR, 'weather_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "#         wfile = WDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "        #usecols=3 to read only the weather summary from 06:00 to 18:00\n",
    "#         fcast_am = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[3])\n",
    "        #usecols=6 to read only the weather summary from 18:00 to 06:00+1\n",
    "        #fcast_pm = pd.read_csv(wfile, skiprows=4, encoding='shift_jisx0213', usecols=[6])\n",
    "\n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "        \n",
    "        \n",
    "#         #get weather summary data, flatten the array and convert to list\n",
    "#         fcast_am = fcast_am.values.flatten().tolist()\n",
    "#         fcast_am = [item.replace('\\u3000','') for item in fcast_am] #remove pesky UNICODE character\n",
    "        \n",
    "#         #convert weather summary to forecast type using dictionary fcast_dict{}\n",
    "#         self.fforecast = np.array([fcast_dict[x] for x in fcast_am])\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        if (tot_day_radiation < 3.5):\n",
    "            day_state = 0\n",
    "        elif (3.5 <= tot_day_radiation < 7):\n",
    "            day_state = 1\n",
    "        elif (7 <= tot_day_radiation < 12):\n",
    "            day_state = 2\n",
    "        elif (12 <= tot_day_radiation < 15):\n",
    "            day_state = 3\n",
    "        elif (15 <= tot_day_radiation < 17.5):\n",
    "            day_state = 4\n",
    "        else:\n",
    "            day_state = 5\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.no_of_day_state = 6;\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "        self.enp = self.BOPT - self.batt\n",
    "#         self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "#         norm_fcast = self.fcast/(self.no_of_day_state-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "#         norm_fcast = self.fcast/(self.no_of_day_state-1)\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "    def rewardfn(self):\n",
    "        R_PARAM = 20000 #chosen empirically for best results\n",
    "        mu = 0\n",
    "        sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "        norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "#         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "#             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "#         else:\n",
    "#             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "        if(self.violation_flag):\n",
    "            norm_reward -= 0.5\n",
    "            \n",
    "        return (norm_reward)\n",
    "        \n",
    "    \n",
    "#     #reward function\n",
    "#     def rewardfn(self):\n",
    "        \n",
    "#         #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "#         bmean = np.mean(self.btrack)\n",
    "#         bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "#         # based on the sigmoid function\n",
    "#         # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "#         p1_sharpness = 10\n",
    "#         n1_sharpness = 20\n",
    "#         shift1 = 0.5\n",
    "#         # r1(x) = 0.5 when x = 0.25. \n",
    "#         # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "#         if(bdev<=0.25): \n",
    "#             r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "#         else: \n",
    "#             r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "#         # r1 ranges from -1 to 1\n",
    "            \n",
    "#         #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "#         if(bdev <=0.25):\n",
    "#             # enp ranges from enp = (0,3) of DMAX\n",
    "#             p2_sharpness = 2\n",
    "#             n2_sharpness = 2\n",
    "#             shift2 = 6    \n",
    "#             # r1(x) = 0.5 when x = 2. \n",
    "#             # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "# #             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "# #             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "# #             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "#             x = np.abs(self.enp/self.DMAX)\n",
    "#             if(x<=2): \n",
    "#                 r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "#             else: \n",
    "#                 r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "#         else:\n",
    "#             r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "#         # r2 ranges from 0 to 1\n",
    "\n",
    "#         #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "#         if(self.violation_flag):\n",
    "#             violation_penalty = 2\n",
    "#         else:\n",
    "#             violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "# #         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "#         return (r1 + r2 - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt <= self.BMIN or self.batt >= self.BMAX ):\n",
    "            if(self.violation_flag == False): \n",
    "                self.violation_counter += 1\n",
    "            self.violation_flag = True #penalty for violating battery limits anytime during the day\n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "        self.enp = self.BOPT - self.batt \n",
    "#         self.enp = self.binit - self.batt\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "\n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward = self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                self.batt = self.BOPT  \n",
    "            \n",
    "            self.violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/5\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*3      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 3 #number of state space parameter [batt, enp, henergy]\n",
    "\n",
    "HIDDEN_LAYER = 200\n",
    "NO_OF_ITERATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        self.fc1.weight.data.normal_(0, 0.01)   # initialization\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.fc2.weight.data.normal_(0, 0.01)   # initialization\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.01)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device = \"cpu\"\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "    \n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] # return the argmax index\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.30\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    norm_batt, norm_enp, norm_henergy = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Iteration 0:  TOKYO, 2009 \n",
      "EPSILON = 0.5, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=   -0.878\n",
      "Violation Counter \t= 199\n",
      "Iteration 1:  TOKYO, 2009 \n",
      "EPSILON = 0.5, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=   -0.866\n",
      "Violation Counter \t= 201\n",
      "Iteration 2:  TOKYO, 2009 \n",
      "EPSILON = 0.5, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=   -0.578\n",
      "Violation Counter \t= 144\n",
      "Iteration 3:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.538\n",
      "Violation Counter \t= 29\n",
      "Iteration 4:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.374\n",
      "Violation Counter \t= 78\n",
      "Iteration 5:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.387\n",
      "Violation Counter \t= 75\n",
      "Iteration 6:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.661\n",
      "Violation Counter \t= 49\n",
      "Iteration 7:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.616\n",
      "Violation Counter \t= 34\n",
      "Iteration 8:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.195\n",
      "Violation Counter \t= 78\n",
      "Iteration 9:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.549\n",
      "Violation Counter \t= 46\n",
      "Iteration 10:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.808\n",
      "Violation Counter \t= 48\n",
      "Iteration 11:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.466\n",
      "Violation Counter \t= 20\n",
      "Iteration 12:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.569\n",
      "Violation Counter \t= 12\n",
      "Iteration 13:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.399\n",
      "Violation Counter \t= 7\n",
      "Iteration 14:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.668\n",
      "Violation Counter \t= 23\n",
      "Iteration 15:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.745\n",
      "Violation Counter \t= 21\n",
      "Iteration 16:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.936\n",
      "Violation Counter \t= 14\n",
      "Iteration 17:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.048\n",
      "Violation Counter \t= 17\n",
      "Iteration 18:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.034\n",
      "Violation Counter \t= 21\n",
      "Iteration 19:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.866\n",
      "Violation Counter \t= 28\n",
      "Iteration 20:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.777\n",
      "Violation Counter \t= 35\n",
      "Iteration 21:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.688\n",
      "Violation Counter \t= 45\n",
      "Iteration 22:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.920\n",
      "Violation Counter \t= 31\n",
      "Iteration 23:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.606\n",
      "Violation Counter \t= 39\n",
      "Iteration 24:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.546\n",
      "Violation Counter \t= 32\n",
      "Iteration 25:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.745\n",
      "Violation Counter \t= 33\n",
      "Iteration 26:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.493\n",
      "Violation Counter \t= 57\n",
      "Iteration 27:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.596\n",
      "Violation Counter \t= 53\n",
      "Iteration 28:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.632\n",
      "Violation Counter \t= 62\n",
      "Iteration 29:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.602\n",
      "Violation Counter \t= 68\n",
      "Iteration 30:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.596\n",
      "Violation Counter \t= 69\n",
      "Iteration 31:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.739\n",
      "Violation Counter \t= 71\n",
      "Iteration 32:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.866\n",
      "Violation Counter \t= 48\n",
      "Iteration 33:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.940\n",
      "Violation Counter \t= 44\n",
      "Iteration 34:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.822\n",
      "Violation Counter \t= 53\n",
      "Iteration 35:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.702\n",
      "Violation Counter \t= 62\n",
      "Iteration 36:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.735\n",
      "Violation Counter \t= 69\n",
      "Iteration 37:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.736\n",
      "Violation Counter \t= 63\n",
      "Iteration 38:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.029\n",
      "Violation Counter \t= 45\n",
      "Iteration 39:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.270\n",
      "Violation Counter \t= 45\n",
      "Iteration 40:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.386\n",
      "Violation Counter \t= 39\n",
      "Iteration 41:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.208\n",
      "Violation Counter \t= 44\n",
      "Iteration 42:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.258\n",
      "Violation Counter \t= 48\n",
      "Iteration 43:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.085\n",
      "Violation Counter \t= 55\n",
      "Iteration 44:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    0.985\n",
      "Violation Counter \t= 65\n",
      "Iteration 45:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.088\n",
      "Violation Counter \t= 55\n",
      "Iteration 46:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.038\n",
      "Violation Counter \t= 55\n",
      "Iteration 47:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.112\n",
      "Violation Counter \t= 47\n",
      "Iteration 48:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.214\n",
      "Violation Counter \t= 39\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "EPSILON = 0.9, LR = 0.001, RST_FLAG = True\n",
      "Average Reward \t\t=    1.252\n",
      "Violation Counter \t= 41\n"
     ]
    }
   ],
   "source": [
    "#TRAIN USING data from TOKYO, WAKKANAI, MINAMIDAITO  FROM 2005 to 2015 on pre-trained model\n",
    "#Simulating actual working\n",
    "#No Random Battery \n",
    "#No battery reset\n",
    "#No Shuffle\n",
    "#No day_balance\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Device used: ' , device)\n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "# old = dqn.eval_net.fc1.weight.data.numpy().flatten()\n",
    "# old2 = old\n",
    "\n",
    "# old2nd = dqn.eval_net.fc2.weight.data.numpy().flatten()\n",
    "# old2nd2 = old2nd\n",
    "\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_flag = True #put CAPM in traimode\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "Y_EAR = 2004\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    if (iteration < MEMORY_CAPACITY/(24*7*4*12) ):\n",
    "        EPSILON = 0.5\n",
    "    else:\n",
    "        EPSILON = 0.9\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = 2009#random.choice(np.arange(2005,2015))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=reset_flag) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('Iteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    print('EPSILON = {}, LR = {}, RST_FLAG = {}'.format(EPSILON, LR, reset_flag))\n",
    "\n",
    "    \n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "                                                       # However, we are interested only in the reward\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] = r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "#             if (capm.BMIN == capm.batt or capm.BMAX == capm.batt):\n",
    "#                 capm.batt = capm.BOPT\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "#             print(\"Learning:\",capm.eno.year, capm.eno.day)\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "    #             print(\"End of Year\")\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[reward_rec != 0] #remove all zero rewards in the middle of the days\n",
    "    print(\"Average Reward \\t\\t= {:8.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_reward_rec = yr_record[:,2]\n",
    "    yr_reward_rec = yr_reward_rec[yr_reward_rec != 0]\n",
    "   \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "    \n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "    \n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     plt.show()\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax0 = fig.add_subplot(131)\n",
    "#     ax0.plot(yr_reward_rec, color='g')\n",
    "    \n",
    "#     ax1 = fig.add_subplot(132)\n",
    "#     new = dqn.eval_net.fc1.weight.data.numpy().flatten()\n",
    "#     ax1.plot(old2,color='r', alpha=0.4)\n",
    "#     ax1.plot(old,color='r')\n",
    "#     ax1.plot(new,color='b')\n",
    "#     old2 = old\n",
    "#     old = new\n",
    "    \n",
    "#     ax2 = fig.add_subplot(133)\n",
    "#     new2nd = dqn.eval_net.fc2.weight.data.numpy().flatten()\n",
    "#     ax2.plot(old2nd2,color='r', alpha=0.4)\n",
    "#     ax2.plot(old2nd,color='r')\n",
    "#     ax2.plot(new2nd,color='b')\n",
    "#     old2nd2 = old2nd\n",
    "#     old2nd = new2nd\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training\n",
    "# Save the last model\n",
    "# torch.save(dqn.eval_net.state_dict(), TFILENAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,365]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0:31:11.083492\n"
     ]
    }
   ],
   "source": [
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #BFILENAME -> loads the best model\n",
    "# #TFILENAME -> loads the last model\n",
    "# MODELFILE = BFILENAME\n",
    "# TEST_LOCATION = 'tokyo'\n",
    "# dqn = DQN()\n",
    "# for TEST_YEAR in np.arange(2015,2018):\n",
    "#     test_model(TEST_LOCATION, TEST_YEAR, MODELFILE, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #BFILENAME -> loads the best model\n",
    "# #TFILENAME -> loads the last model\n",
    "# MODELFILE = TFILENAME\n",
    "# TEST_LOCATION = 'tokyo'\n",
    "# dqn = DQN()\n",
    "# for TEST_YEAR in np.arange(2016,2018)\n",
    "#     test_model(TEST_LOCATION, TEST_YEAR, MODELFILE, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_YEAR = 2018\n",
    "# for TEST_LOCATION in ['tokyo','wakkanai','minamidaito']:\n",
    "#     test_model(TEST_LOCATION, TEST_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn = DQN()\n",
    "# capm = CAPM(TEST_LOCATION,TEST_YEAR, shuffle=False, trainmode=False) #if reset is True\n",
    "# capm.eno = ENO(TEST_LOCATION,TEST_YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "# capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "\n",
    "# # load the required model\n",
    "# dqn.eval_net.load_state_dict(torch.load(MODELFILE))\n",
    "# dqn.eval_net.eval()\n",
    "# print('Model Used: ',MODELFILE)\n",
    "\n",
    "# s, r, day_end, year_end = capm.reset()\n",
    "# yr_test_record = np.empty(4)\n",
    "\n",
    "# while True:\n",
    "#     a = dqn.choose_greedy_action(s)\n",
    "\n",
    "#     #state = [batt, enp, henergy, fcast]\n",
    "#     yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "#     # take action\n",
    "#     s_, r, day_end, year_end = capm.step(a)\n",
    "    \n",
    "# #     if day_end:\n",
    "# #         if (capm.BMIN ==capm.batt or capm.batt == capm.BMAX):\n",
    "# #             capm.batt = capm.BOPT\n",
    "        \n",
    "\n",
    "#     if year_end:\n",
    "#         print(\"End of Test\")\n",
    "#         break\n",
    "\n",
    "#     s = s_\n",
    "\n",
    "# yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "# #Plot the reward and battery for the entire year run\n",
    "# title = TEST_LOCATION.upper() + ',' + str(TEST_YEAR)\n",
    "\n",
    "# NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "# yr_test_reward_rec = yr_test_record[:,2]\n",
    "# yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "# print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(24,10))\n",
    "# fig.suptitle(title, fontsize=15)\n",
    "\n",
    "# #     ax1 = fig.add_subplot(211)\n",
    "# #     ax1.plot(yr_test_reward_rec)\n",
    "# #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "# #     ax1.set_ylim([-3,1])\n",
    "\n",
    "# ax2 = fig.add_subplot(111)\n",
    "# ax2.plot(yr_test_record[:,0],'r')\n",
    "# ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "# ax2.set_ylim([0,1])\n",
    "# plt.sca(ax2)\n",
    "# plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = TEST_LOCATION.upper() + ',' + str(TEST_YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(150,170):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*yr_test_record[START,0],'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
